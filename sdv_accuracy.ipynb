{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8129575,
          "sourceType": "datasetVersion",
          "datasetId": 4804958
        }
      ],
      "dockerImageVersionId": 30684,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "8kgckvLoRel-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing/data manipulation\n",
        "import numpy as np\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.metrics import make_scorer, accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T22:32:04.460477Z",
          "iopub.execute_input": "2024-04-16T22:32:04.460862Z",
          "iopub.status.idle": "2024-04-16T22:32:07.649924Z",
          "shell.execute_reply.started": "2024-04-16T22:32:04.460833Z",
          "shell.execute_reply": "2024-04-16T22:32:07.648696Z"
        },
        "trusted": true,
        "id": "xmA8HnwURel_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read CSVs\n",
        "\n",
        "For the purposes of this project, we will evaluate the results of the synthesized dataset \"VAE_synthetic\" and comparing it to the original dataset \"condensed_dataset\".\n",
        "\n",
        "We will do this by first training various classification models on the synthetic dataset, and then fitting"
      ],
      "metadata": {
        "id": "vQucQHfRRemA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv('/content/condensed_dataset.csv')\n",
        "\n",
        "train_data = pd.read_csv('/content/VAE_Synthetic.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T22:32:10.191694Z",
          "iopub.execute_input": "2024-04-16T22:32:10.192412Z",
          "iopub.status.idle": "2024-04-16T22:32:14.346983Z",
          "shell.execute_reply.started": "2024-04-16T22:32:10.192374Z",
          "shell.execute_reply": "2024-04-16T22:32:14.345467Z"
        },
        "trusted": true,
        "id": "Pf63At_IRemA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting and Pre-processing Dataset\n",
        "\n",
        "Here, we begin pre-processing the dataset.\n",
        "\n",
        "We first check the DataFrame for any NA values. Once complete, we then split the dataset, stratify the target variable, as well as add a standard scaler. The dataset will be split 80/20 for training/testing. Missing data imputations are conducted as well, using median values so as to mitigate both outlier and skewed data influence."
      ],
      "metadata": {
        "id": "HTgHRG0MRemA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT0_WAw50rvF",
        "outputId": "4b23e4d6-54b5-43a0-8da2-c12c7430826a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 23 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   dt           500 non-null    int64  \n",
            " 1   switch       500 non-null    int64  \n",
            " 2   src          500 non-null    object \n",
            " 3   dst          500 non-null    object \n",
            " 4   pktcount     500 non-null    int64  \n",
            " 5   bytecount    500 non-null    int64  \n",
            " 6   dur          500 non-null    int64  \n",
            " 7   dur_nsec     500 non-null    int64  \n",
            " 8   tot_dur      500 non-null    float64\n",
            " 9   flows        500 non-null    int64  \n",
            " 10  packetins    500 non-null    int64  \n",
            " 11  pktperflow   500 non-null    int64  \n",
            " 12  byteperflow  500 non-null    int64  \n",
            " 13  pktrate      500 non-null    int64  \n",
            " 14  Pairflow     500 non-null    int64  \n",
            " 15  Protocol     500 non-null    object \n",
            " 16  port_no      500 non-null    int64  \n",
            " 17  tx_bytes     500 non-null    int64  \n",
            " 18  rx_bytes     500 non-null    int64  \n",
            " 19  tx_kbps      500 non-null    int64  \n",
            " 20  rx_kbps      500 non-null    int64  \n",
            " 21  tot_kbps     500 non-null    int64  \n",
            " 22  label        500 non-null    int64  \n",
            "dtypes: float64(1), int64(19), object(3)\n",
            "memory usage: 90.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NAs in entire DataFrame\n",
        "print(train_data.isnull().values.any())\n",
        "\n",
        "# Check for NAs in the columns\n",
        "print(train_data.isnull().any())\n",
        "\n",
        "# Check for NAs in the rows\n",
        "print(train_data.isnull().any(axis=1))\n",
        "\n",
        "# Check for null values in DataFrame\n",
        "na_ct = train_data.isnull().values.flatten().sum()\n",
        "\n",
        "# Count number of False values\n",
        "non_na_ct = train_data.size - na_ct\n",
        "\n",
        "print(\"Number of True Values (NAs):\", na_ct)\n",
        "print(\"Number of False values (Non-NAs):\", non_na_ct)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T22:32:15.086112Z",
          "iopub.execute_input": "2024-04-16T22:32:15.086515Z",
          "iopub.status.idle": "2024-04-16T22:32:15.262215Z",
          "shell.execute_reply.started": "2024-04-16T22:32:15.086485Z",
          "shell.execute_reply": "2024-04-16T22:32:15.260189Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCaKu40fRemA",
        "outputId": "abe934d7-5eae-4aa8-badc-113fe15b88e4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "dt             False\n",
            "switch         False\n",
            "src            False\n",
            "dst            False\n",
            "pktcount       False\n",
            "bytecount      False\n",
            "dur            False\n",
            "dur_nsec       False\n",
            "tot_dur        False\n",
            "flows          False\n",
            "packetins      False\n",
            "pktperflow     False\n",
            "byteperflow    False\n",
            "pktrate        False\n",
            "Pairflow       False\n",
            "Protocol       False\n",
            "port_no        False\n",
            "tx_bytes       False\n",
            "rx_bytes       False\n",
            "tx_kbps        False\n",
            "rx_kbps        False\n",
            "tot_kbps       False\n",
            "label          False\n",
            "dtype: bool\n",
            "0      False\n",
            "1      False\n",
            "2      False\n",
            "3      False\n",
            "4      False\n",
            "       ...  \n",
            "495    False\n",
            "496    False\n",
            "497    False\n",
            "498    False\n",
            "499    False\n",
            "Length: 500, dtype: bool\n",
            "Number of True Values (NAs): 0\n",
            "Number of False values (Non-NAs): 11500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mapping dictionary for all categorical columns\n",
        "mapping_dict = {}\n",
        "\n",
        "for col in train_data.columns:\n",
        "    if train_data[col].dtype == 'object':\n",
        "        mapping = {label: idx for idx, label in enumerate(np.unique(train_data[col]))}\n",
        "        mapping_dict[col] = mapping\n",
        "\n",
        "for col, mapping in mapping_dict.items():\n",
        "    train_data[col] = train_data[col].map(mapping)"
      ],
      "metadata": {
        "id": "OJWxAXb5p7I5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Data\n",
        "X = train_data.drop(columns =['label'], axis = 1)\n",
        "y = train_data['label']\n",
        "\n",
        "# Stratified train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T22:32:20.200536Z",
          "iopub.execute_input": "2024-04-16T22:32:20.201019Z",
          "iopub.status.idle": "2024-04-16T22:32:20.809621Z",
          "shell.execute_reply.started": "2024-04-16T22:32:20.200986Z",
          "shell.execute_reply": "2024-04-16T22:32:20.808206Z"
        },
        "trusted": true,
        "id": "Y__RqgEuRemB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding standard scaling to X_train and X_test\n",
        "sc = StandardScaler()\n",
        "\n",
        "X_train_scaled = sc.fit_transform(X_train)\n",
        "X_test_scaled = sc.transform(X_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T22:32:22.103240Z",
          "iopub.execute_input": "2024-04-16T22:32:22.103642Z",
          "iopub.status.idle": "2024-04-16T22:32:22.535825Z",
          "shell.execute_reply.started": "2024-04-16T22:32:22.103615Z",
          "shell.execute_reply": "2024-04-16T22:32:22.534603Z"
        },
        "trusted": true,
        "id": "Jg3uvs07RemB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Data\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_train_imputed = imputer.fit_transform(X_train_scaled)\n",
        "X_test_imputed = imputer.transform(X_test_scaled)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T22:32:23.789789Z",
          "iopub.execute_input": "2024-04-16T22:32:23.790238Z",
          "iopub.status.idle": "2024-04-16T22:32:26.805053Z",
          "shell.execute_reply.started": "2024-04-16T22:32:23.790204Z",
          "shell.execute_reply": "2024-04-16T22:32:26.803815Z"
        },
        "trusted": true,
        "id": "MWroERdzRemB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Employing Classification Methods on Training Dataset\n",
        "\n",
        "After pre-processing is complete, we then begin running the training dataset through each classification method.\n",
        "\n",
        "The purpose of this project will be to determine the quality of the fitted model generated from the synthesized dataset with the highest quality from our previous project. As such, six classification methods will be utilized and evaluated in order to determine which one will be the most effective at evaluating the final test dataset.\n",
        "\n",
        "We first introduce each classifier method and then conduct cross-validation to check the robustness of each model. For the purpose of this project, 5 folds will be used, save for SVM.\n",
        "\n",
        "The accuracy scores of each will be shown as an output to compare, with the mean accuracy score of the 5 splits being used as final metric to choose the best model."
      ],
      "metadata": {
        "id": "zNxBY2EURemB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n",
        "\n",
        "Logistic Regression scores show that this model may be suitable as the primary classification method for the test dataset. It performs almost as well as SVM, which can be an indicator that the decision boundary between classes are approximately linear. With a mean accuracy score of 0.97, this was one of the better methods overall."
      ],
      "metadata": {
        "id": "RnkNsr7JRemC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Introduce model\n",
        "lr_clf = LogisticRegression(max_iter=2000, penalty='l2', C=1.0, random_state=42)\n",
        "\n",
        "# Cross-validation\n",
        "cv_lr = cross_val_score(lr_clf, X_train_imputed, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print Accuracy scores\n",
        "print(\"Cross-validation Accuracy scores:\", cv_lr)\n",
        "print(\"Mean Accuracy score:\", cv_lr.mean())"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQHdiR75RemC",
        "outputId": "7455dfe9-4921-4bb0-f5fd-87d806cea838"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation Accuracy scores: [1.     0.9375 0.975  0.9625 0.975 ]\n",
            "Mean Accuracy score: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM\n",
        "\n",
        "**This classification method was chosen**, as it had the highest overall accuracy score among the tested classifier methods. This would suggest that it might be the best fit for this specific dataset among all the models tested."
      ],
      "metadata": {
        "id": "Whn7px1URemC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This classifier was chosen due to having the highest mean accuracy score overall\n",
        "\n",
        "# Create a pipeline for SVM and Standard Scaling\n",
        "pipe_svc = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    SVC(random_state=42),\n",
        ")\n",
        "\n",
        "# Define parameter distributions for random search\n",
        "param_range = [0.01, 1.0, 100.0]\n",
        "\n",
        "param_grid = [{'svc__C': param_range,\n",
        "              'svc__kernel': ['linear']},\n",
        "             {'svc__C': param_range,\n",
        "             'svc__gamma': param_range,\n",
        "             'svc__kernel': ['rbf']}]\n",
        "\n",
        "# Creating Randomized Search, setting estimators, parameters, and maximum CPU usage\n",
        "rs = RandomizedSearchCV(estimator=pipe_svc, param_distributions=param_grid, scoring='accuracy', refit=True, n_iter=5, cv=5, random_state=1, n_jobs=-1)\n",
        "rs.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Get the best parameters and best accuracy score\n",
        "best_params = rs.best_params_\n",
        "best_score = rs.best_score_\n",
        "\n",
        "# Print Accuracy scores\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Accuracy Score:\", best_score)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T20:40:05.123128Z",
          "iopub.execute_input": "2024-04-16T20:40:05.123624Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNSrnRWURemC",
        "outputId": "00d9b618-4a94-48d6-9717-a36edc7fb94a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'svc__kernel': 'linear', 'svc__C': 1.0}\n",
            "Best Accuracy Score: 0.9724999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree\n",
        "\n",
        "Decision Tree achieved the lowest accuracy score out of all methods chosen, with a score of approximately 0.939. This would suggest that the dataset may be noisy or complex, and that a single decision may not be enough to sufficiently capture such nuances. Additionally, a decision tree's tendency to overfitting would also lend to reasons why it has the lowest score."
      ],
      "metadata": {
        "id": "rwDtzKE7RemC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Introduce model\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Cross-validation\n",
        "cv_dt = cross_val_score(dt_clf, X_train_imputed, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print Accuracy scores\n",
        "print(\"Cross-validation Accuracy Scores:\", cv_dt)\n",
        "print(\"Mean Accuracy score:\", cv_dt.mean())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T22:32:37.678417Z",
          "iopub.execute_input": "2024-04-16T22:32:37.678845Z",
          "iopub.status.idle": "2024-04-16T22:33:32.089985Z",
          "shell.execute_reply.started": "2024-04-16T22:32:37.678813Z",
          "shell.execute_reply": "2024-04-16T22:33:32.088650Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bQgyHAYRemC",
        "outputId": "61529c49-1182-42b6-b13b-7e8fc15a96c4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation Accuracy Scores: [0.975  0.9125 0.95   0.95   0.9125]\n",
            "Mean Accuracy score: 0.9399999999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Nearest Neighbors\n",
        "\n",
        "KNN was able to perform well, achieving a mean accuracy score of 0.96. This suggests that the class distribution within the dataset allows for effective classification based on proximity. However, when compared to the other models, KNN is slightly edged out in terms of robustness."
      ],
      "metadata": {
        "id": "tPwzbRf3RemC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Introduce model\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
        "\n",
        "# Cross-validation\n",
        "cv_knn = cross_val_score(knn_clf, X_train_imputed, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print Accuracy scores\n",
        "print(\"Cross-validation Accuracy Scores:\", cv_knn)\n",
        "print(\"Mean Accuracy score:\", cv_knn.mean())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T22:34:18.658181Z",
          "iopub.execute_input": "2024-04-16T22:34:18.658703Z",
          "iopub.status.idle": "2024-04-16T22:35:01.985606Z",
          "shell.execute_reply.started": "2024-04-16T22:34:18.658673Z",
          "shell.execute_reply": "2024-04-16T22:35:01.984657Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXxZ7AqgRemC",
        "outputId": "3f943977-4b1e-45f6-c006-038a24cb7f51"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation Accuracy Scores: [0.975  0.9375 0.95   0.9625 0.975 ]\n",
            "Mean Accuracy score: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest\n",
        "\n",
        " While one decision tree alone may not be enough to produce an adequate accuracy score, having multiple decision trees would allow for a significant improvement in predictions. Utilizing Random Forest yielded a result of approximately 0.96749. By leveraging multiple decision trees, less variance is generated, and increases the overall accuracy score compared to one single tree.\n",
        "\n",
        "While it may not have produced the highest score, this method would still be considered with larger and more complex datasets."
      ],
      "metadata": {
        "id": "4wTQR-SIRemC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Introduce model, setting additional parameters for n number of trees and maximum CPU usage\n",
        "rf_clf = RandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=42)\n",
        "\n",
        "# Cross-validation\n",
        "cv_rf = cross_val_score(rf_clf, X_train_imputed, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print Accuracy scores\n",
        "print(\"Cross-validation Accuracy Scores:\", cv_rf)\n",
        "print(\"Mean Accuracy score:\", cv_rf.mean())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T22:35:01.987583Z",
          "iopub.execute_input": "2024-04-16T22:35:01.988330Z",
          "iopub.status.idle": "2024-04-16T22:38:55.361518Z",
          "shell.execute_reply.started": "2024-04-16T22:35:01.988293Z",
          "shell.execute_reply": "2024-04-16T22:38:55.359895Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NGIrtDBRemC",
        "outputId": "b7bd8f91-71f0-425e-fda7-54486c0e96d2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation Accuracy Scores: [0.9875 0.95   0.9625 0.9625 0.975 ]\n",
            "Mean Accuracy score: 0.9674999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost\n",
        "\n",
        "Our last classification method, XGBoost, yielded approximately 0.965. While this result may be adequate on its own and may perform better on larger and more complex datasets, SVM was able to slightly edge out XGBoost's score."
      ],
      "metadata": {
        "id": "h0BPcEm6PWSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Introduce model\n",
        "xgb_clf = xgb.XGBClassifier(random_state=42)\n",
        "\n",
        "# Cross-validation\n",
        "cv_xgb = cross_val_score(xgb_clf, X_train_imputed, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print Accuracy scores\n",
        "print(\"Cross-validation Accuracy Scores:\", cv_xgb)\n",
        "print(\"Mean Accuracy score:\", cv_xgb.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XPGcQb_rIJk",
        "outputId": "0c9e1221-f05e-416f-fefa-990e6c4fd3ed"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation Accuracy Scores: [0.9875 0.95   0.9625 0.9625 0.9625]\n",
            "Mean Accuracy score: 0.9650000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Comparison At-a-Glance\n",
        "\n",
        "Six models were tested. SVM performed the best, with Logistic Regression close behind.\n",
        "The performance of each model is the following:\n",
        "    \n",
        "| Model    | Accuracy Score |\n",
        "| :--------: | :--------------------: |\n",
        "| Logistic Regression | 0.97           |\n",
        "| SVM    | **0.9724999999999999**          |\n",
        "| Decision Tree    | 0.9399999999999998          |\n",
        "| KNN    | 0.96           |\n",
        "| Random Forest    | 0.9674999999999999           |\n",
        "| XGBoost  | 0.9650000000000001|\n",
        "\n",
        "With these results, we conclude that SVM has the best results, and thus will utilize SVM for our final model."
      ],
      "metadata": {
        "id": "uC1dFio_7ADs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Classifer Method for Test Dataset\n",
        "\n",
        "Here, we re-create the code to pre-process the training and testing dataset to be evaluated by the SVM algorithm. The code remains mostly the same as earlier, with the biggest differences being the encoding the results of the label to a new column."
      ],
      "metadata": {
        "id": "41G8iXa-RemC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mapping dictionary for all categorical columns\n",
        "mapping_dict = {}\n",
        "\n",
        "# Creating mapping dictionary for training data\n",
        "for col in train_data.columns:\n",
        "    if train_data[col].dtype == 'object':\n",
        "        mapping = {label: idx for idx, label in enumerate(np.unique(train_data[col]))}\n",
        "        mapping_dict[col] = mapping\n",
        "\n",
        "for col, mapping in mapping_dict.items():\n",
        "    train_data[col] = train_data[col].map(mapping)\n",
        "\n",
        "# Creating mapping dictionary for testing data\n",
        "for col in test_data.columns:\n",
        "    if test_data[col].dtype == 'object':\n",
        "        mapping = {label: idx for idx, label in enumerate(np.unique(test_data[col]))}\n",
        "        mapping_dict[col] = mapping\n",
        "\n",
        "for col, mapping in mapping_dict.items():\n",
        "    test_data[col] = test_data[col].map(mapping)\n",
        "\n",
        "# Training Data\n",
        "X_train = train_data.drop(columns =['label'], axis = 1)\n",
        "y_train = train_data['label']\n",
        "\n",
        "# Test Data\n",
        "X_test = test_data.drop(columns =['label'], axis = 1)\n",
        "y_test = test_data['label']\n",
        "\n",
        "# Adding standard scaling to X_train and X_test\n",
        "sc = StandardScaler()\n",
        "\n",
        "X_train_scaled = sc.fit_transform(X_train)\n",
        "X_test_scaled = sc.transform(X_test)\n",
        "\n",
        "# Handling Missing Data\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_train_imputed = imputer.fit_transform(X_train_scaled)\n",
        "X_test_imputed = imputer.transform(X_test_scaled)"
      ],
      "metadata": {
        "trusted": true,
        "id": "MiBKvx_ZRemD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Testing Dataset and Generating CSV File\n",
        "\n",
        "Once the data from the training and testing dataset have been pre-processed, the testing dataset is then evaluated with our chosen classifer method, of which the results will be converted to a csv file."
      ],
      "metadata": {
        "id": "kTsdtPouRemD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the training data\n",
        "\n",
        "# Define parameter distributions for random search\n",
        "param_range = [0.01, 1.0, 100.0]\n",
        "\n",
        "param_grid = [{'svc__C': param_range,\n",
        "              'svc__kernel': ['linear']},\n",
        "             {'svc__C': param_range,\n",
        "             'svc__gamma': param_range,\n",
        "             'svc__kernel': ['rbf']}]\n",
        "\n",
        "# Creating Randomized Search, setting estimators, parameters, and maximum CPU usage\n",
        "rs = RandomizedSearchCV(estimator=pipe_svc, param_distributions=param_grid, scoring='accuracy', refit=True, n_iter=5, cv=5, random_state=1, n_jobs=-1)\n",
        "rs.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Predict the fitted model on the test data\n",
        "rs_pred = rs.predict(X_test_imputed)\n",
        "\n",
        "# Check accuracy of the test data\n",
        "accuracy = accuracy_score(y_test, rs_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Create a new DataFrame with the original test data and replace the 'label' column\n",
        "result = test_data.copy()\n",
        "result['label'] = rs_pred\n",
        "\n",
        "# Save the result DataFrame to a CSV file\n",
        "result.to_csv('result.csv', index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "TB7_HJTmRemD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5540b3d6-bd56-420d-8432-de785b81aa70"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Visualization\n",
        "\n",
        "Once the csv file has been generated, a final check and visualization of the DataFrame is conducted before submission.  \n",
        "\n"
      ],
      "metadata": {
        "id": "RYaVJkk7RemD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv('/content/result.csv')\n",
        "print(submission)\n",
        "\n",
        "value_counts = submission['label'].value_counts()\n",
        "print(value_counts)"
      ],
      "metadata": {
        "trusted": true,
        "id": "d3Fwqf-TRemD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f09c5d-1533-4a61-e221-a173a3e2c76e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        dt  switch  src  dst  pktcount  bytecount  dur   dur_nsec  \\\n",
            "0    11425       1    0    1     45304   48294064  100  716000000   \n",
            "1    11605       1    0    1    126395  134737070  280  734000000   \n",
            "2    11425       1    1    1     90333   96294978  200  744000000   \n",
            "3    11425       1    1    1     90333   96294978  200  744000000   \n",
            "4    11425       1    1    1     90333   96294978  200  744000000   \n",
            "..     ...     ...  ...  ...       ...        ...  ...        ...   \n",
            "995   9966       1    0    0     59972   63930152  133  252000000   \n",
            "996   9966       1    0    0     59972   63930152  133  252000000   \n",
            "997   9966       1    0    0     59972   63930152  133  252000000   \n",
            "998   9966       1    0    0     59972   63930152  133  252000000   \n",
            "999   9936       1    0    0     46440   49505040  103  248000000   \n",
            "\n",
            "          tot_dur  flows  ...  pktrate  Pairflow  Protocol  port_no  \\\n",
            "0    1.010000e+11      3  ...      451         0         0        3   \n",
            "1    2.810000e+11      2  ...      451         0         0        4   \n",
            "2    2.010000e+11      3  ...      451         0         0        1   \n",
            "3    2.010000e+11      3  ...      451         0         0        2   \n",
            "4    2.010000e+11      3  ...      451         0         0        3   \n",
            "..            ...    ...  ...      ...       ...       ...      ...   \n",
            "995  1.330000e+11      2  ...      451         0         0        2   \n",
            "996  1.330000e+11      2  ...      451         0         0        2   \n",
            "997  1.330000e+11      2  ...      451         0         0        4   \n",
            "998  1.330000e+11      2  ...      451         0         0        3   \n",
            "999  1.030000e+11      2  ...      450         0         0        2   \n",
            "\n",
            "      tx_bytes  rx_bytes  tx_kbps  rx_kbps  tot_kbps  label  \n",
            "0    143928631      3917        0        0         0      0  \n",
            "1         3842      3520        0        0         0      0  \n",
            "2         3795      1242        0        0         0      0  \n",
            "3         3688      1492        0        0         0      0  \n",
            "4         3413      3665        0        0         0      0  \n",
            "..         ...       ...      ...      ...       ...    ...  \n",
            "995       3689      1402        0        0         0      0  \n",
            "996       3649      1242        0        0         0      0  \n",
            "997       3539      3413        0        0         0      0  \n",
            "998       3413      3539        0        0         0      0  \n",
            "999       3579      1172        0        0         0      0  \n",
            "\n",
            "[1000 rows x 23 columns]\n",
            "label\n",
            "0    880\n",
            "1    120\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "There are some key takeaways that were gathered from this project.\n",
        "\n",
        "The first is that a generally high mean accuracy score was maintained across all the models tested. This suggests that the dataset was generated in a way that maintains the underlying patterns of the original dataset. In other words, this indicates that the synthetic dataset was a good proxy for the original data - at least, for these models.\n",
        "\n",
        "Another takeaway was that with models such as Logistic Regression and SVM performing similarly in terms of accuracy suggests that the dataset has well-separated classes in the feature space - SVM, especially.\n",
        "\n",
        "Finally, the high mean accuracy scores across all the models would indicate that the synthetic data is of high quality, and that it was able to successfully preserve the relationships and features that were present in the original dataset. Thus, we can conclude that the synthetic dataset is useful for model testing and validation."
      ],
      "metadata": {
        "id": "iMBGR82i2CSi"
      }
    }
  ]
}